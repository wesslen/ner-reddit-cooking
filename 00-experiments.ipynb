{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4393db05",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "Question: What datasets will provide the best performing model as measured by F1 accuracy?\n",
        "\n",
        "### 1. Experiment 1: GPT3 by itself\n",
        "\n",
        "Let's start first with using GPT3.5 zeroshot data only. For simplicity, we'll allow `prodigy train` to split for the evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9adbd58c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created unstructured dataset 'hmwk_1_zeroshot' in database SQLite\u001b[0m\n",
            "\u001b[38;5;2m✔ Imported 500 annotated examples and saved them to 'hmwk_1_zeroshot'\n",
            "(session 2024-02-14_11-05-16) in database SQLite\u001b[0m\n",
            "Found and keeping existing \"answer\" in 0 examples\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy db-in hmwk_1_zeroshot data/gpt3-5-zeroshot.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "290de66e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 11:05:18,442] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 400 | Evaluation: 100 (20% split)\n",
            "Training: 400 | Evaluation: 100\n",
            "Labels: ner (3)\n",
            "[2024-02-14 11:05:18,533] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2024-02-14 11:05:18,535] [INFO] Created vocabulary\n",
            "[2024-02-14 11:05:18,535] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 11:05:19,315] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 400 | Evaluation: 100 (20% split)\n",
            "Training: 400 | Evaluation: 100\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     34.21    0.00    0.00    0.00    0.00\n",
            "  1     200       1427.13   3203.68   17.30   25.76   13.03    0.17\n",
            "  2     400        515.59   2571.07   22.37   26.88   19.16    0.22\n",
            "  4     600        218.53   2227.98   31.21   36.60   27.20    0.31\n",
            "  5     800        834.12   1686.16   34.10   42.77   28.35    0.34\n",
            "  8    1000        514.53   1495.52   32.20   39.44   27.20    0.32\n",
            " 10    1200        621.01   1049.14   30.25   36.81   25.67    0.30\n",
            " 13    1400        600.63    724.20   29.83   41.22   23.37    0.30\n",
            " 17    1600        892.35    629.23   32.09   37.63   27.97    0.32\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-1/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   53.49   31.51   39.66\n",
            "DISH         35.48   30.56   32.84\n",
            "EQUIPMENT    24.00   13.95   17.65\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_zeroshot ./output/experiment-1 --label-stats --training.patience=800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "88aa7ab1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Train curve diagnostic ===========================\u001b[0m\n",
            "Training 4 times with 25%, 50%, 75%, 100% of the data\n",
            "\n",
            "%      Score    ner   \n",
            "----   ------   ------\n",
            "  0%   0.00     0.00  \n",
            " 25%   0.24 ▲   0.24 ▲\n",
            " 50%   0.29 ▲   0.29 ▲\n",
            " 75%   0.30 ▲   0.30 ▲\n",
            "100%   0.34 ▲   0.34 ▲\n",
            "\n",
            "\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n",
            "As a rule of thumb, if accuracy increases in the last segment, this could\n",
            "indicate that collecting more annotations of the same type will improve the\n",
            "model further.\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train-curve --ner hmwk_1_zeroshot --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "041cd4b2",
      "metadata": {},
      "source": [
        "This gives us evidence that we can do better. But before adding more training data, let's add a dedicated hold out (evaluation) dataset. This is to allow us to have a consistent evaluation dataset across all of our subsequent experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f945f3d",
      "metadata": {},
      "source": [
        "### 2. Experiment 2: GPT3 with holdout\n",
        "\n",
        "Let's now load an evaluation dataset (200 unlabeled), retrain on `hmwk-1-zeroshot` and eval on `hmwk-1-eval`.\n",
        "\n",
        "For the evaluation dataset, I used a different data 200 data. So in your case you may use `ner.manual` or `ner.correct`, but in my case I only needed to load the data like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a1c28883",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created unstructured dataset 'hmwk_1_eval' in database SQLite\u001b[0m\n",
            "\u001b[38;5;2m✔ Imported 200 annotated examples and saved them to 'hmwk_1_eval'\n",
            "(session 2024-02-14_11-10-37) in database SQLite\u001b[0m\n",
            "Found and keeping existing \"answer\" in 200 examples\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy db-in hmwk_1_eval data/eval-reddit.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2880d4b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 11:10:39,029] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 500 | Evaluation: 200 (from datasets)\n",
            "Training: 500 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "[2024-02-14 11:10:39,139] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2024-02-14 11:10:39,141] [INFO] Created vocabulary\n",
            "[2024-02-14 11:10:39,141] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 11:10:40,001] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 500 | Evaluation: 200 (from datasets)\n",
            "Training: 500 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     48.64    0.00    0.00    0.00    0.00\n",
            "  0     200        222.98   3569.07    4.77   87.50    2.45    0.05\n",
            "  1     400        168.44   2221.45   32.55   41.87   26.62    0.33\n",
            "  3     600        365.82   2348.88   28.80   33.26   25.39    0.29\n",
            "  4     800        404.08   2263.78   39.25   42.31   36.60    0.39\n",
            "  6    1000        398.03   1732.58   40.60   47.64   35.38    0.41\n",
            "  8    1200        670.26   1588.29   34.14   42.45   28.55    0.34\n",
            " 11    1400        667.35   1242.29   35.65   45.63   29.25    0.36\n",
            " 14    1600        632.51    858.33   35.21   43.44   29.60    0.35\n",
            " 17    1800        748.35    742.73   30.41   35.78   26.44    0.30\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-2/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   59.77   40.62   48.37\n",
            "EQUIPMENT    40.38   28.00   33.07\n",
            "DISH         22.52   22.32   22.42\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_zeroshot,eval:hmwk_1_eval ./output/experiment-2  --label-stats --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0505830e",
      "metadata": {},
      "source": [
        "Interestingly, the **F1 score** went up to 0.41 from 0.36. We need to be cautious before assigning causation (e.g., was it the increase in training size from 400 to 500? was it differences in the evaluation dataset?). It's not clear, but for the rest of the experiments we'll keep the 0.41 as our benchmark.\n",
        "\n",
        "It's also worth noting that `INGREDIENT` has a much higher accuracy than `EQUIPMENT` and especially `DISH`. Any ideas on why that could be? What does it mean when Precision (P) is much higher than Recall (R)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef2dd7ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Train curve diagnostic ===========================\u001b[0m\n",
            "Training 4 times with 25%, 50%, 75%, 100% of the data\n",
            "\n",
            "%      Score    ner   \n",
            "----   ------   ------\n",
            "  0%   0.00     0.00  \n",
            " 25%   0.32 ▲   0.32 ▲\n",
            " 50%   0.34 ▲   0.34 ▲\n",
            " 75%   0.34     0.34  \n",
            "100%   0.41 ▲   0.41 ▲\n",
            "\n",
            "\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n",
            "As a rule of thumb, if accuracy increases in the last segment, this could\n",
            "indicate that collecting more annotations of the same type will improve the\n",
            "model further.\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train-curve --ner hmwk_1_zeroshot,eval:hmwk_1_eval --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0294597",
      "metadata": {},
      "source": [
        "We can also run `train-curve` to see if we've \"leveled off\", i.e., what's the extra value from adding more training data.\n",
        "\n",
        "It seems that we should consider adding more training data. But before we do - let's train with only the Workshop data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc25491",
      "metadata": {},
      "source": [
        "\n",
        "### 3. Experiment 3: Workshop with holdout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c2252c",
      "metadata": {},
      "source": [
        "Let's now train only with the workshop data, but keep the same evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5d447704",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created unstructured dataset 'hmwk_1_workshop' in database SQLite\u001b[0m\n",
            "\u001b[38;5;2m✔ Imported 1250 annotated examples and saved them to 'hmwk_1_workshop'\n",
            "(session 2024-02-14_11-16-56) in database SQLite\u001b[0m\n",
            "Found and keeping existing \"answer\" in 1250 examples\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy db-in hmwk_1_workshop data/pydata-nyc-2023.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c80bb1",
      "metadata": {},
      "source": [
        "Train with only the workshop data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a89f1f9c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 11:16:58,517] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1183 | Evaluation: 200 (from datasets)\n",
            "Training: 1163 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "[2024-02-14 11:16:58,727] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2024-02-14 11:16:58,729] [INFO] Created vocabulary\n",
            "[2024-02-14 11:16:58,729] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 11:17:00,293] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1183 | Evaluation: 200 (from datasets)\n",
            "Training: 1163 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     69.71    0.68    0.98    0.53    0.01\n",
            "  0     200         77.06   3192.50   40.50   57.19   31.35    0.40\n",
            "  0     400        217.71   2327.05   51.01   53.56   48.69    0.51\n",
            "  1     600       1685.13   2451.77   55.47   57.90   53.24    0.55\n",
            "  2     800        207.56   2148.05   55.66   54.95   56.39    0.56\n",
            "  2    1000        282.18   2318.89   59.65   56.19   63.57    0.60\n",
            "  3    1200       2497.70   2351.78   58.90   59.86   57.97    0.59\n",
            "  5    1400        496.68   2170.87   58.82   60.86   56.92    0.59\n",
            "  6    1600        578.82   1907.44   59.51   63.67   55.87    0.60\n",
            "  8    1800        726.92   1952.51   60.70   60.28   61.12    0.61\n",
            " 10    2000        763.63   1533.63   62.48   62.26   62.70    0.62\n",
            " 12    2200       1419.61   1324.71   60.51   59.10   62.00    0.61\n",
            " 16    2400        908.75   1075.90   60.29   57.90   62.87    0.60\n",
            " 19    2600       9297.66   1070.28   61.32   61.70   60.95    0.61\n",
            " 23    2800        978.46    811.19   59.59   63.62   56.04    0.60\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-3/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   65.92   69.01   67.43\n",
            "EQUIPMENT    62.50   60.00   61.22\n",
            "DISH         47.52   42.86   45.07\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_workshop,eval:hmwk_1_eval ./output/experiment-3 --label-stats --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13cff092",
      "metadata": {},
      "source": [
        "We find that the workshop data as the training evaluates much better (**0.62** F1 score) on my evaluation dataset. Therefore, it's safe to say that the Workshop data can better predict my annotations than the GPT3 labeled trained model.\n",
        "\n",
        "That's a big jump so there seems to be a lot of value in the Workshop Data. But it's also important to note that the Workshop dataset is almost 2x larger than the GPT3.5 dataset, so it's not clear: is it quantity or quality?\n",
        "\n",
        "Notice that `INGREDIENT` accuracy, especially recall, improved a lot as did `EQUIPMENT` and `DISH`.\n",
        "\n",
        "Let's now run `train-curve` to see if there's evidence that we may improve model performance if we label some more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b0aeca3b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Train curve diagnostic ===========================\u001b[0m\n",
            "Training 4 times with 25%, 50%, 75%, 100% of the data\n",
            "\n",
            "%      Score    ner   \n",
            "----   ------   ------\n",
            "  0%   0.01     0.01  \n",
            " 25%   0.48 ▲   0.48 ▲\n",
            " 50%   0.55 ▲   0.55 ▲\n",
            " 75%   0.61 ▲   0.61 ▲\n",
            "100%   0.62 ▲   0.62 ▲\n",
            "\n",
            "\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n",
            "As a rule of thumb, if accuracy increases in the last segment, this could\n",
            "indicate that collecting more annotations of the same type will improve the\n",
            "model further.\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train-curve --ner hmwk_1_workshop,eval:hmwk_1_eval --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc793fb",
      "metadata": {},
      "source": [
        "### 4. Experiment 4: Workshop & GPT as training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8b2f3e",
      "metadata": {},
      "source": [
        "Let's now experiment with adding the GPT3 Data, combining it to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f52451cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created dataset 'hmwk_1_train_exp4'\u001b[0m\n",
            "\u001b[38;5;2m✔ Merged 1750 examples from 2 datasets\u001b[0m\n",
            "Created merged dataset 'hmwk_1_train_exp4'\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy db-merge hmwk_1_zeroshot,hmwk_1_workshop hmwk_1_train_exp4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "85e22838",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 11:20:21,280] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1683 | Evaluation: 200 (from datasets)\n",
            "Training: 1577 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "[2024-02-14 11:20:21,558] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2024-02-14 11:20:21,560] [INFO] Created vocabulary\n",
            "[2024-02-14 11:20:21,560] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 11:20:23,584] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1683 | Evaluation: 200 (from datasets)\n",
            "Training: 1577 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     39.86    0.00    0.00    0.00    0.00\n",
            "  0     200         92.69   3360.64   25.27   56.36   16.29    0.25\n",
            "  0     400         79.39   2126.62   42.32   54.72   34.50    0.42\n",
            "  1     600        190.51   2763.95   53.46   58.26   49.39    0.53\n",
            "  1     800        204.86   2688.44   55.58   55.83   55.34    0.56\n",
            "  2    1000        535.26   2818.91   56.98   60.39   53.94    0.57\n",
            "  2    1200        372.82   3024.98   56.58   57.50   55.69    0.57\n",
            "  3    1400        486.73   3307.68   53.97   62.24   47.64    0.54\n",
            "  4    1600        703.33   3235.16   56.08   56.48   55.69    0.56\n",
            "  5    1800        934.44   3656.35   58.44   59.28   57.62    0.58\n",
            "  7    2000       1011.86   3478.30   57.50   57.90   57.09    0.57\n",
            "  9    2200       1368.08   3357.62   57.17   59.32   55.17    0.57\n",
            " 11    2400       1683.55   3000.56   59.62   62.17   57.27    0.60\n",
            " 13    2600       1962.80   2827.89   56.37   56.17   56.57    0.56\n",
            " 16    2800       1797.06   2188.30   56.97   55.69   58.32    0.57\n",
            " 18    3000       1778.83   1845.11   52.85   52.99   52.71    0.53\n",
            " 21    3200       1892.55   1666.83   52.64   55.91   49.74    0.53\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-4/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   65.07   63.54   64.30\n",
            "EQUIPMENT    58.93   44.00   50.38\n",
            "DISH         52.63   44.64   48.31\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_train_exp4,eval:hmwk_1_eval ./output/experiment-4 --label-stats --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d39f1c4",
      "metadata": {},
      "source": [
        "Interestingly, we found that actually including the GPT3 annotated dataset **decreases** accuracy slightly. We need to be careful about inferring significance, but this provides evidence that we may be better of not using the GPT3 data at all!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9011e2",
      "metadata": {},
      "source": [
        "### 5. Experiment 5: Correct GPT3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f690ac3a",
      "metadata": {},
      "source": [
        "To test this idea, let's now examine some of the GPT3.5 data. By reviewing we can not only correct some data, we can possibly develop insight to when the model is performing inconsistently. But to do so, it would be helpful to see the GPT3.5 labels **and** our best model (experiment 2) predictions at the same time.\n",
        "\n",
        "Luckily, there's a `model-as-annotator` Prodigy recipe that allows you to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afd65ac",
      "metadata": {},
      "source": [
        "I've commented out below because I previously saved this step as `data/hmwk-1-review.jsonl`. This allows me to fully rerun the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "703454df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting labels from the 'ner' component\n",
            "Using 3 labels: ['DISH', 'EQUIPMENT', 'INGREDIENT']\n",
            "100%|████████████████████████████████████████| 500/500 [00:01<00:00, 376.48it/s]\n"
          ]
        }
      ],
      "source": [
        "!prodigy ner.model-annotate hmwk_1_workshop_model ./output/experiment-3/model-best data/gpt3-5-zeroshot.jsonl ner_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cc62896d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 3 label(s): DISH, EQUIPMENT, INGREDIENT\n",
            "Added dataset hmwk_1_review to database SQLite.\n",
            "\n",
            "✨  Starting the web server at http://localhost:8080 ...\n",
            "Open the app in your browser and start annotating!\n",
            "\n",
            "^C\n",
            "\n",
            "\u001b[38;5;2m✔ Saved 100 annotations to database SQLite\u001b[0m\n",
            "Dataset: hmwk_1_review\n",
            "Session ID: 2024-02-14_14-30-25\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy review hmwk_1_review hmwk_1_workshop_model,hmwk_1_zeroshot --label DISH,EQUIPMENT,INGREDIENT --auto-accept\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9bede1d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m prodigy db-out hmwk_1_review > data/hmwk-1-review.jsonl "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ebbc883e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     177 data/hmwk-1-review.jsonl\n"
          ]
        }
      ],
      "source": [
        "!wc -l data/hmwk-1-review.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3167e9",
      "metadata": {},
      "source": [
        "Notice that while we saved 100 examples, we have 77. That's because we choose `--auto-accept` so we also automatically kept any examples where our model and GPT3 annotation agreed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "52cf80e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Created dataset 'hmwk_1_train_exp5'\u001b[0m\n",
            "\u001b[38;5;2m✔ Merged 1427 examples from 2 datasets\u001b[0m\n",
            "Created merged dataset 'hmwk_1_train_exp5'\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy db-merge hmwk_1_workshop,hmwk_1_review hmwk_1_train_exp5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "91b5ec26",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 14:51:25,874] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1360 | Evaluation: 200 (from datasets)\n",
            "Training: 1263 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "[2024-02-14 14:51:26,130] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2024-02-14 14:51:26,132] [INFO] Created vocabulary\n",
            "[2024-02-14 14:51:26,133] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 14:51:27,826] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1360 | Evaluation: 200 (from datasets)\n",
            "Training: 1263 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     74.50    0.00    0.00    0.00    0.00\n",
            "  0     200        350.62   3292.15   34.78   47.85   27.32    0.35\n",
            "  0     400        615.09   2428.28   48.30   52.58   44.66    0.48\n",
            "  1     600        161.48   2171.68   53.12   52.67   53.59    0.53\n",
            "  1     800        348.34   2308.28   57.24   57.04   57.44    0.57\n",
            "  2    1000        601.17   2181.36   59.45   60.25   58.67    0.59\n",
            "  3    1200        563.58   2278.52   56.89   57.76   56.04    0.57\n",
            "  4    1400       5775.79   2317.99   59.73   58.24   61.30    0.60\n",
            "  5    1600        715.00   2269.79   60.91   60.70   61.12    0.61\n",
            "  7    1800        651.68   2037.85   61.23   60.00   62.52    0.61\n",
            "  9    2000        818.62   1918.42   59.85   57.84   62.00    0.60\n",
            " 11    2200        954.83   1551.80   56.73   55.54   57.97    0.57\n",
            " 14    2400        961.21   1294.12   61.54   60.75   62.35    0.62\n",
            " 17    2600       1099.99   1021.33   59.00   60.07   57.97    0.59\n",
            " 20    2800       1329.02    855.44   59.09   58.99   59.19    0.59\n",
            " 24    3000        993.86    743.53   58.64   59.32   57.97    0.59\n",
            " 27    3200       1159.19    672.91   59.54   62.21   57.09    0.60\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-5/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   65.43   69.01   67.17\n",
            "EQUIPMENT    46.59   54.67   50.31\n",
            "DISH         53.76   44.64   48.78\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_train_exp5,eval:hmwk_1_eval ./output/experiment-5 --label-stats --training.patience=800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "802e33a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Train curve diagnostic ===========================\u001b[0m\n",
            "Training 4 times with 25%, 50%, 75%, 100% of the data\n",
            "\n",
            "%      Score    ner   \n",
            "----   ------   ------\n",
            "  0%   0.00     0.00  \n",
            " 25%   0.48 ▲   0.48 ▲\n",
            " 50%   0.58 ▲   0.58 ▲\n",
            " 75%   0.60 ▲   0.60 ▲\n",
            "100%   0.62 ▲   0.62 ▲\n",
            "\n",
            "\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n",
            "As a rule of thumb, if accuracy increases in the last segment, this could\n",
            "indicate that collecting more annotations of the same type will improve the\n",
            "model further.\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train-curve --ner hmwk_1_train_exp5,eval:hmwk_1_eval --training.patience=800"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5406e191",
      "metadata": {},
      "source": [
        "### 6. Modify Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf9a7e2",
      "metadata": {},
      "source": [
        "We're going to follow [Prodigy docs](https://prodi.gy/docs/named-entity-recognition#transfer-learning) to improve the model by using word vectors (embeddings). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "eb7a59f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.11/site-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.25.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "409c1659",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Generating Prodigy config =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-generating config with spaCy\u001b[0m\n",
            "\u001b[38;5;4mℹ Using config from base model\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated training config\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2024-02-14 15:14:46,481] [INFO] Set up nlp object from config\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1183 | Evaluation: 200 (from datasets)\n",
            "Training: 1163 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "[2024-02-14 15:14:46,894] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "[2024-02-14 15:14:46,894] [INFO] Resuming training for: ['ner', 'tok2vec']\n",
            "[2024-02-14 15:14:46,902] [INFO] Created vocabulary\n",
            "[2024-02-14 15:14:48,420] [INFO] Added vectors: en_core_web_lg\n",
            "[2024-02-14 15:14:49,431] [INFO] Finished initializing nlp object\n",
            "[2024-02-14 15:14:49,431] [INFO] Initialized pipeline components: []\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "Components: ner\n",
            "Merging training and evaluation data for 1 components\n",
            "  - [ner] Training: 1183 | Evaluation: 200 (from datasets)\n",
            "Training: 1163 | Evaluation: 200\n",
            "Labels: ner (3)\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler',\n",
            "'lemmatizer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Frozen components: ['tagger', 'parser', 'attribute_ruler',\n",
            "'lemmatizer']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SPEED   SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------  ------\n",
            "  0       0          0.00     39.11    0.00    0.00    0.00  5119.62    0.00\n",
            "  2    1000          0.00  10884.36   65.81   64.75   66.90  5811.09    0.66\n",
            " 10    2000          0.00  11434.33   64.74   63.02   66.55  5760.54    0.65\n",
            " 26    3000          0.00   6749.23   64.01   62.11   66.02  5857.31    0.64\n",
            " 44    4000          0.00   2691.94   63.89   62.05   65.85  5796.19    0.64\n",
            " 62    5000          0.00   1921.97   64.01   62.11   66.02  5770.66    0.64\n",
            " 79    6000          0.00   1469.31   64.00   62.25   65.85  5608.81    0.64\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/experiment-6/model-last\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "INGREDIENT   69.85   72.40   71.10\n",
            "EQUIPMENT    56.58   57.33   56.95\n",
            "DISH         52.59   54.46   53.51\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m prodigy train --ner hmwk_1_workshop,eval:hmwk_1_eval ./output/experiment-6 --base-model en_core_web_lg --label-stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87dae9fb",
      "metadata": {},
      "source": [
        "### Save models as wheel files\n",
        "\n",
        "As an alternative to zip files, I'll save the models as wheel files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1340b62b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-1/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-1.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-1.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-1.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-1.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-1.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-1.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-1.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-1.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-1.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-1.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-1.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-1.0.0/dist/en_ner_reddit_cooking-1.0.0-py3-none-any.whl\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-2/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-2.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-2.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-2.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-2.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-2.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-2.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-2.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-2.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-2.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-2.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-2.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-2.0.0/dist/en_ner_reddit_cooking-2.0.0-py3-none-any.whl\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-3/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-3.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-3.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-3.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-3.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-3.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-3.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-3.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-3.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-3.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-3.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-3.0.0/dist/en_ner_reddit_cooking-3.0.0-py3-none-any.whl\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-4/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-4.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-4.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-4.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-4.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-4.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-4.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-4.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-4.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-4.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-4.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-4.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-4.0.0/dist/en_ner_reddit_cooking-4.0.0-py3-none-any.whl\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-5/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-5.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-5.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-5.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-5.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-5.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-5.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-5.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-5.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-5.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-5.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-5.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-5.0.0/dist/en_ner_reddit_cooking-5.0.0-py3-none-any.whl\n",
            "\u001b[38;5;4mℹ Building package artifacts: wheel\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "output/experiment-6/model-best/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory\n",
            "'en_ner_reddit_cooking-6.0.0'\u001b[0m\n",
            "packages/en_ner_reddit_cooking-6.0.0\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/en_ner_reddit_cooking\n",
            "copying en_ner_reddit_cooking/__init__.py -> build/lib/en_ner_reddit_cooking\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tokenizer -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/config.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/README.md -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/meta.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler/patterns -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/moves -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/lookups.bin -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/strings.json -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/key2row -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors.cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "creating build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/cfg -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/model -> build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying en_ner_reddit_cooking/meta.json -> build/lib/en_ner_reddit_cooking\n",
            "/usr/local/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "installing to build/bdist.macosx-13-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.macosx-13-x86_64\n",
            "creating build/bdist.macosx-13-x86_64/wheel\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tokenizer -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler/patterns -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/moves -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/lookups.bin -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/strings.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/key2row -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/config.cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/README.md -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/cfg -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/model -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger\n",
            "copying build/lib/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0\n",
            "copying build/lib/en_ner_reddit_cooking/__init__.py -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "copying build/lib/en_ner_reddit_cooking/meta.json -> build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking\n",
            "running install_egg_info\n",
            "running egg_info\n",
            "creating en_ner_reddit_cooking.egg-info\n",
            "writing en_ner_reddit_cooking.egg-info/PKG-INFO\n",
            "writing dependency_links to en_ner_reddit_cooking.egg-info/dependency_links.txt\n",
            "writing entry points to en_ner_reddit_cooking.egg-info/entry_points.txt\n",
            "writing requirements to en_ner_reddit_cooking.egg-info/requires.txt\n",
            "writing top-level names to en_ner_reddit_cooking.egg-info/top_level.txt\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'en_ner_reddit_cooking.egg-info/SOURCES.txt'\n",
            "Copying en_ner_reddit_cooking.egg-info to build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-6.0.0-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.macosx-13-x86_64/wheel/en_ner_reddit_cooking-6.0.0.dist-info/WHEEL\n",
            "creating 'dist/en_ner_reddit_cooking-6.0.0-py3-none-any.whl' and adding 'build/bdist.macosx-13-x86_64/wheel' to it\n",
            "adding 'en_ner_reddit_cooking/__init__.py'\n",
            "adding 'en_ner_reddit_cooking/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/README.md'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/config.cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/meta.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tokenizer'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/attribute_ruler/patterns'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/lemmatizer/lookups/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/ner/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/parser/moves'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tagger/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/cfg'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/tok2vec/model'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/key2row'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/lookups.bin'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/strings.json'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors'\n",
            "adding 'en_ner_reddit_cooking/en_ner_reddit_cooking-6.0.0/vocab/vectors.cfg'\n",
            "adding 'en_ner_reddit_cooking-6.0.0.dist-info/METADATA'\n",
            "adding 'en_ner_reddit_cooking-6.0.0.dist-info/WHEEL'\n",
            "adding 'en_ner_reddit_cooking-6.0.0.dist-info/entry_points.txt'\n",
            "adding 'en_ner_reddit_cooking-6.0.0.dist-info/top_level.txt'\n",
            "adding 'en_ner_reddit_cooking-6.0.0.dist-info/RECORD'\n",
            "removing build/bdist.macosx-13-x86_64/wheel\n",
            "\u001b[38;5;2m✔ Successfully created binary wheel\u001b[0m\n",
            "packages/en_ner_reddit_cooking-6.0.0/dist/en_ner_reddit_cooking-6.0.0-py3-none-any.whl\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p packages\n",
        "!python -m spacy package ./output/experiment-1/model-best ./packages --name ner_reddit_cooking --version 1.0.0 --build wheel\n",
        "!python -m spacy package ./output/experiment-2/model-best ./packages --name ner_reddit_cooking --version 2.0.0 --build wheel\n",
        "!python -m spacy package ./output/experiment-3/model-best ./packages --name ner_reddit_cooking --version 3.0.0 --build wheel\n",
        "!python -m spacy package ./output/experiment-4/model-best ./packages --name ner_reddit_cooking --version 4.0.0 --build wheel\n",
        "!python -m spacy package ./output/experiment-5/model-best ./packages --name ner_reddit_cooking --version 5.0.0 --build wheel\n",
        "!python -m spacy package ./output/experiment-6/model-best ./packages --name ner_reddit_cooking --version 6.0.0 --build wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2052d6a4",
      "metadata": {},
      "source": [
        "Let's now copy `cp` the files to the `models` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6c93e099",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p models\n",
        "!cp -n packages/en_ner_reddit_cooking-1.0.0/dist/en_ner_reddit_cooking-1.0.0-py3-none-any.whl models/en_ner_reddit_cooking-1.0.0-py3-none-any.whl\n",
        "!cp -n packages/en_ner_reddit_cooking-2.0.0/dist/en_ner_reddit_cooking-2.0.0-py3-none-any.whl models/en_ner_reddit_cooking-2.0.0-py3-none-any.whl\n",
        "!cp -n packages/en_ner_reddit_cooking-3.0.0/dist/en_ner_reddit_cooking-3.0.0-py3-none-any.whl models/en_ner_reddit_cooking-3.0.0-py3-none-any.whl\n",
        "!cp -n packages/en_ner_reddit_cooking-4.0.0/dist/en_ner_reddit_cooking-4.0.0-py3-none-any.whl models/en_ner_reddit_cooking-4.0.0-py3-none-any.whl\n",
        "!cp -n packages/en_ner_reddit_cooking-5.0.0/dist/en_ner_reddit_cooking-5.0.0-py3-none-any.whl models/en_ner_reddit_cooking-5.0.0-py3-none-any.whl\n",
        "!cp -n packages/en_ner_reddit_cooking-6.0.0/dist/en_ner_reddit_cooking-6.0.0-py3-none-any.whl models/en_ner_reddit_cooking-6.0.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e01c495",
      "metadata": {},
      "source": [
        "This now means that you can install any of the models using `pip install` like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e2d41b86",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./models/en_ner_reddit_cooking-3.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: spacy<3.7.0,>=3.6.1 in /usr/local/lib/python3.11/site-packages (from en-ner-reddit-cooking==3.0.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (1.25.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.1->en-ner-reddit-cooking==3.0.0) (2.1.3)\n",
            "Installing collected packages: en-ner-reddit-cooking\n",
            "Successfully installed en-ner-reddit-cooking-3.0.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install models/en_ner_reddit_cooking-3.0.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7074cbec",
      "metadata": {},
      "source": [
        "Then you can use in Python to load that model like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2a107528",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(mayo, plain yogurt, curry powder, mustard powder, mustard, ground coriander)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_ner_reddit_cooking\")\n",
        "doc = nlp(\"Make a dressing with mayo, plain yogurt, curry powder, mustard powder, mustard, ground coriander.\")\n",
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ec9e1c",
      "metadata": {},
      "source": [
        "### Save models to HF Hub\n",
        "\n",
        "You can then save your model to Hugging Face Hub too. For this, you'll need to have installed `spacy-huggingface_hub`, which is in the `requirements.txt`. You'll also need a HF Hub/Spaces account and set up your API token (see [link](https://huggingface.co/settings/tokens))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24708931",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!huggingface-cli login # recommend doing in terminal, not notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6fb45bd",
      "metadata": {},
      "source": [
        "I want to only push my v3 model as it's lightweight (6MB), it's fast and still has good performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "acd55064",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Publishing to repository 'wesslen/en_ner_reddit_cooking'\u001b[0m\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/meta.json en_ner_reddit_cooking/meta.json\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/README.md en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/README.md\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/config.cfg en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/config.cfg\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/meta.json en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/meta.json\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tokenizer en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tokenizer\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/cfg en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/cfg\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/model en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/model\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/moves en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/ner/moves\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/cfg en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/cfg\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/model en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/tok2vec/model\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/key2row en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/key2row\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/lookups.bin en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/lookups.bin\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/strings.json en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/strings.json\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors.cfg en_ner_reddit_cooking/en_ner_reddit_cooking-3.0.0/vocab/vectors.cfg\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking-3.0.0.dist-info/METADATA en_ner_reddit_cooking-3.0.0.dist-info/METADATA\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking-3.0.0.dist-info/WHEEL en_ner_reddit_cooking-3.0.0.dist-info/WHEEL\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking-3.0.0.dist-info/entry_points.txt en_ner_reddit_cooking-3.0.0.dist-info/entry_points.txt\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking-3.0.0.dist-info/top_level.txt en_ner_reddit_cooking-3.0.0.dist-info/top_level.txt\n",
            "result /var/folders/bc/gqn95mfn5t1dmm0t3bp_t9nr0000gn/T/tmp_q9xxbyg/en_ner_reddit_cooking-3.0.0.dist-info/RECORD en_ner_reddit_cooking-3.0.0.dist-info/RECORD\n",
            "\u001b[38;5;2m✔ Extracted information from .whl file\u001b[0m\n",
            "\u001b[38;5;2m✔ Created model card\u001b[0m\n",
            "en_ner_reddit_cooking (v3.0.0)\n",
            "Pushing repository to the hub...\n",
            "en_ner_reddit_cooking-any-py3-none-any.whl:   0%|   | 0.00/5.72M [00:00<?, ?B/s]\n",
            "Upload 3 LFS files:   0%|                                 | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "model:   0%|                                        | 0.00/6.01M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "lookups.bin:   0%|                                   | 0.00/1.00 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model:   0%|                                | 16.4k/6.01M [00:00<00:36, 163kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "en_ner_reddit_cooking-any-py3-none-any.whl:   3%| | 147k/5.72M [00:00<00:04, 1.3\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "lookups.bin: 100%|████████████████████████████| 1.00/1.00 [00:00<00:00, 4.79B/s]\u001b[A\u001b[A\n",
            "en_ner_reddit_cooking-any-py3-none-any.whl:   5%| | 279k/5.72M [00:00<00:04, 1.1\n",
            "\n",
            "en_ner_reddit_cooking-any-py3-none-any.whl:  61%|▌| 3.51M/5.72M [00:00<00:00, 13\u001b[A\u001b[A\n",
            "\n",
            "en_ner_reddit_cooking-any-py3-none-any.whl: 100%|█| 5.72M/5.72M [00:00<00:00, 7.\u001b[A\u001b[A\n",
            "model: 100%|███████████████████████████████| 6.01M/6.01M [00:00<00:00, 8.06MB/s]\n",
            "\n",
            "Upload 3 LFS files: 100%|█████████████████████████| 3/3 [00:00<00:00,  3.23it/s]\u001b[A\n",
            "\u001b[38;5;2m✔ Pushed repository 'en_ner_reddit_cooking' to the hub\u001b[0m\n",
            "\n",
            "View your model here:\n",
            "https://huggingface.co/wesslen/en_ner_reddit_cooking\n",
            "\n",
            "Install your model:\n",
            "pip install https://huggingface.co/wesslen/en_ner_reddit_cooking/resolve/main/en_ner_reddit_cooking-any-py3-none-any.whl\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy huggingface-hub push models/en_ner_reddit_cooking-3.0.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c92050e",
      "metadata": {},
      "source": [
        "If it runs successfully, you should get a final message like:\n",
        "```\n",
        "View your model here:https://huggingface.co/wesslen/en_ner_reddit_cooking\n",
        "\n",
        "Install your model:\n",
        "pip install https://huggingface.co/wesslen/en_ner_reddit_cooking/resolve/main/en_ner_reddit_cooking-any-py3-none-any.whl\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010118b",
      "metadata": {},
      "source": [
        "You can view and test my model here: https://huggingface.co/wesslen/en_ner_reddit_cooking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cfd8fac",
      "metadata": {},
      "source": [
        "### Appendix: Drop files\n",
        "\n",
        "This is commented out and can be used to \"clean\" any Prodigy datasets and remove any models/packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "a1a60571",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m prodigy drop hmwk_1_zeroshot\n",
        "# !python -m prodigy drop hmwk_1_workshop\n",
        "# !python -m prodigy drop hmwk_1_eval\n",
        "# !python -m prodigy drop hmwk_1_train_exp3\n",
        "# !python -m prodigy drop hmwk_1_train_exp4\n",
        "# !python -m prodigy drop hmwk_1_train_exp5\n",
        "# !rm -rf output\n",
        "# !rm -rf packages"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
